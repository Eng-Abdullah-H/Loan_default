{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11060896,"sourceType":"datasetVersion","datasetId":6891692}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/engabdullahhegazy/virsion-loan-default?scriptVersionId=228627937\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Loan Default Prediction Project\n\n## Project Overview\nThis project aims to analyze and predict loan default risks using a real-world dataset. The dataset consists of **255,347** loan applications with **18** features, including demographic details, financial history, and loan attributes. Our goal is to provide insights into the factors influencing loan defaults and build a predictive model to assist financial institutions in making informed lending decisions.\n\n## Dataset Description\nThe dataset includes the following key features:\n- **LoanID**: Unique identifier for each loan\n- **Age**: Borrower’s age\n- **Income**: Annual income of the borrower\n- **LoanAmount**: Amount of loan requested\n- **CreditScore**: Credit score of the borrower\n- **MonthsEmployed**: Number of months employed\n- **NumCreditLines**: Number of active credit lines\n- **InterestRate**: Interest rate assigned to the loan\n- **LoanTerm**: Duration of the loan in months\n- **DTIRatio**: Debt-to-Income ratio\n- **Education**: Highest education level attained\n- **EmploymentType**: Type of employment (Full-time, Part-time, etc.)\n- **MaritalStatus**: Marital status of the borrower\n- **HasMortgage**: Whether the borrower has an existing mortgage (Yes/No)\n- **HasDependents**: Whether the borrower has dependents (Yes/No)\n- **LoanPurpose**: Purpose of the loan (Auto, Business, Other, etc.)\n- **HasCoSigner**: Whether the loan has a co-signer (Yes/No)\n- **Default**: Target variable indicating if the borrower defaulted (1 = Default, 0 = No Default)\n\n## Initial Analysis\n- The dataset contains **no missing values**, ensuring a clean start for analysis.\n- The age range of borrowers spans from **18 to 69 years**.\n- Loan amounts vary widely, from **$5,000 to $249,999**.\n- Credit scores range between **300 and 849**, providing significant variation.\n- The average debt-to-income (DTI) ratio is approximately **0.5**, indicating varying levels of financial stability among borrowers.\n- Around **11.6%** of loans in the dataset have resulted in default.\n\n## Next Steps\n1. Perform exploratory data analysis (EDA) to uncover trends and correlations.\n2. Engineer relevant features to improve predictive accuracy.\n3. Train machine learning models to classify loan defaults.\n4. Optimize the model for better performance and interpretability.\n\nThis analysis will help financial institutions mitigate risks and make data-driven lending decisions. Further insights will be shared as we progress.\n\n","metadata":{}},{"cell_type":"code","source":"#lets start with data overview and some statistics and informations \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/loan-default/Loan_default.csv\")\n\n# Display the first five rows\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:26.629014Z","iopub.execute_input":"2025-03-17T12:32:26.629406Z","iopub.status.idle":"2025-03-17T12:32:27.250291Z","shell.execute_reply.started":"2025-03-17T12:32:26.629375Z","shell.execute_reply":"2025-03-17T12:32:27.249199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:29.431678Z","iopub.execute_input":"2025-03-17T12:32:29.432089Z","iopub.status.idle":"2025-03-17T12:32:29.438638Z","shell.execute_reply.started":"2025-03-17T12:32:29.432056Z","shell.execute_reply":"2025-03-17T12:32:29.437521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display basic information about the dataset\nprint(df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:30.274229Z","iopub.execute_input":"2025-03-17T12:32:30.274637Z","iopub.status.idle":"2025-03-17T12:32:30.390093Z","shell.execute_reply.started":"2025-03-17T12:32:30.274605Z","shell.execute_reply":"2025-03-17T12:32:30.388868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values\nprint(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:32.574999Z","iopub.execute_input":"2025-03-17T12:32:32.575376Z","iopub.status.idle":"2025-03-17T12:32:32.685383Z","shell.execute_reply.started":"2025-03-17T12:32:32.575323Z","shell.execute_reply":"2025-03-17T12:32:32.684123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show basic statistics for numerical features\ndf.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:34.325946Z","iopub.execute_input":"2025-03-17T12:32:34.32642Z","iopub.status.idle":"2025-03-17T12:32:34.448517Z","shell.execute_reply.started":"2025-03-17T12:32:34.326383Z","shell.execute_reply":"2025-03-17T12:32:34.447393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show basic statistics for categorical features\ndf.describe(include=\"object\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:35.93242Z","iopub.execute_input":"2025-03-17T12:32:35.932771Z","iopub.status.idle":"2025-03-17T12:32:36.422485Z","shell.execute_reply.started":"2025-03-17T12:32:35.932745Z","shell.execute_reply":"2025-03-17T12:32:36.421285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the distribution of the target variable (Default)\nprint(df[\"Default\"].value_counts(normalize=True) * 100)  # Percentage format","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:38.010601Z","iopub.execute_input":"2025-03-17T12:32:38.010952Z","iopub.status.idle":"2025-03-17T12:32:38.019761Z","shell.execute_reply.started":"2025-03-17T12:32:38.010923Z","shell.execute_reply":"2025-03-17T12:32:38.018444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lets plot this imbalanced distribution \nplt.figure(figsize=(6, 4))\nsns.barplot(x=df[\"Default\"].value_counts().index, y=df[\"Default\"].value_counts(), palette=\"Blues\")\nplt.title(\"Class Distribution of Default\")\nplt.xlabel(\"Default (No = 0, yes = 1)\")\nplt.ylabel(\"Count\")\nplt.xticks([0, 1], [\"No Default (0)\", \"Default (1)\"])\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:39.599821Z","iopub.execute_input":"2025-03-17T12:32:39.600159Z","iopub.status.idle":"2025-03-17T12:32:39.764975Z","shell.execute_reply.started":"2025-03-17T12:32:39.600133Z","shell.execute_reply":"2025-03-17T12:32:39.763656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lets make some visualizations \n#Here is the visualization for the integer features.\n# Visualizing only the integer features\nint_columns = df.select_dtypes(include=['int64']).columns.tolist()\n\n# Plot integer features\ndf[int_columns].hist(bins=30, edgecolor='black', layout=(2, 4), figsize=(12, 8))\nplt.suptitle(\"Integer Features Distribution\", fontsize=14)\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:41.448158Z","iopub.execute_input":"2025-03-17T12:32:41.448546Z","iopub.status.idle":"2025-03-17T12:32:43.132204Z","shell.execute_reply.started":"2025-03-17T12:32:41.448516Z","shell.execute_reply":"2025-03-17T12:32:43.130972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Here is the visualization for the object (categorical) features.\n\n# identifying object (categorical) columns\nobject_columns = df.select_dtypes(include=['object']).columns.tolist()\n\n# Plot categorical features (showing top categories for each feature)\nfig, axes = plt.subplots(len(object_columns), 1, figsize=(12, 3 * len(object_columns)))\n\nfor i, col in enumerate(object_columns):\n    value_counts = df[col].value_counts().head(10)  # Show top 10 categories only\n    sns.barplot(x=value_counts.index, y=value_counts.values, ax=axes[i])\n    axes[i].set_title(f\"Top Categories in {col}\")\n    axes[i].set_xticklabels(value_counts.index, rotation=45)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:46.368936Z","iopub.execute_input":"2025-03-17T12:32:46.369287Z","iopub.status.idle":"2025-03-17T12:32:48.038592Z","shell.execute_reply.started":"2025-03-17T12:32:46.36926Z","shell.execute_reply":"2025-03-17T12:32:48.0373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Box plot for float features to detect outliers\n# identifying float columns\nfloat_columns = df.select_dtypes(include=['float64']).columns.tolist()\n\n# Box plot for float features to detect outliers\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df[float_columns])\nplt.title(\"Box Plot of Float Features\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:52.2211Z","iopub.execute_input":"2025-03-17T12:32:52.221496Z","iopub.status.idle":"2025-03-17T12:32:52.417827Z","shell.execute_reply.started":"2025-03-17T12:32:52.221465Z","shell.execute_reply":"2025-03-17T12:32:52.416946Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing\n\nNow that we have explored and visualized the dataset, the next step is **data preprocessing** to ensure data quality before building models. This includes:\n\n1. **Handling Missing Values** – Identify and manage any missing or null values.\n2. **Dealing with Outliers** – Address extreme values that could affect model performance.\n3. **Encoding Categorical Variables** – Convert object (categorical) features into numerical representations.\n4. **Feature Scaling** – Normalize or standardize numerical features if necessary.\n5. **Handling Class Imbalance** – Apply techniques such as oversampling, undersampling, or SMOTE if needed.\n\nLet's start with handling missing values.\n","metadata":{}},{"cell_type":"code","source":"# Check missing values\nmissing_values = df.isnull().sum()\nprint(\"Missing Values:\\n\", missing_values[missing_values > 0])\n\n# Check if there are any hidden missing values (like empty strings)\nprint((df == \"\").sum())  # Count empty strings\nprint(df.isin([\"NA\", \"N/A\", \"null\", \"None\"]).sum())  # Count special missing indicators\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:32:59.384268Z","iopub.execute_input":"2025-03-17T12:32:59.384659Z","iopub.status.idle":"2025-03-17T12:33:00.106258Z","shell.execute_reply.started":"2025-03-17T12:32:59.38463Z","shell.execute_reply":"2025-03-17T12:33:00.105058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Dealing with Outliers \n# Function to detect outliers using IQR\ndef detect_outliers(df, columns):\n    outlier_counts = {}\n    for col in columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n        outlier_counts[col] = len(outliers)\n    return outlier_counts\n\n# Detect outliers in float and integer columns\noutliers = detect_outliers(df, float_columns + int_columns)\nprint(\"Outlier counts per column:\", outliers)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:02.357926Z","iopub.execute_input":"2025-03-17T12:33:02.358266Z","iopub.status.idle":"2025-03-17T12:33:02.48785Z","shell.execute_reply.started":"2025-03-17T12:33:02.358239Z","shell.execute_reply":"2025-03-17T12:33:02.486555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in object_columns:\n    print(f\"{col}: {df[col].unique()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:03.982967Z","iopub.execute_input":"2025-03-17T12:33:03.983374Z","iopub.status.idle":"2025-03-17T12:33:04.13114Z","shell.execute_reply.started":"2025-03-17T12:33:03.983323Z","shell.execute_reply":"2025-03-17T12:33:04.130011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.columns) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:05.83442Z","iopub.execute_input":"2025-03-17T12:33:05.83477Z","iopub.status.idle":"2025-03-17T12:33:05.840511Z","shell.execute_reply.started":"2025-03-17T12:33:05.834743Z","shell.execute_reply":"2025-03-17T12:33:05.839364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[[\"EmploymentType\", \"MaritalStatus\", \"LoanPurpose\"]] = df[\n    [\"EmploymentType\", \"MaritalStatus\", \"LoanPurpose\"]\n].astype(str)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:07.10591Z","iopub.execute_input":"2025-03-17T12:33:07.106259Z","iopub.status.idle":"2025-03-17T12:33:07.128967Z","shell.execute_reply.started":"2025-03-17T12:33:07.106232Z","shell.execute_reply":"2025-03-17T12:33:07.127948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Encoding Categorical Variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# Drop LoanID as it's not useful for modeling\ndf.drop(columns=[\"LoanID\"], inplace=True, errors=\"ignore\")\n\n# Label Encoding for 'Education'\neducation_order = {\"High School\": 0, \"Bachelor's\": 1, \"Master's\": 2, \"PhD\": 3}\ndf[\"Education\"] = df[\"Education\"].map(education_order)\n\n# One-Hot Encoding for nominal categorical variables\ndf = pd.get_dummies(df, columns=[\"EmploymentType\", \"MaritalStatus\", \"LoanPurpose\"], drop_first=True)\n\n# Convert binary categorical variables to 0 and 1\nbinary_columns = [\"HasMortgage\", \"HasDependents\", \"HasCoSigner\"]\ndf[binary_columns] = df[binary_columns].apply(lambda x: x.map({\"No\": 0, \"Yes\": 1}))\n\n# Print final dataframe structure\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:09.158363Z","iopub.execute_input":"2025-03-17T12:33:09.158691Z","iopub.status.idle":"2025-03-17T12:33:09.36532Z","shell.execute_reply.started":"2025-03-17T12:33:09.158666Z","shell.execute_reply":"2025-03-17T12:33:09.36426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(y.dtype)  # Should be int or categorical\nprint(y.unique())  # Should show only 0 and 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:11.972977Z","iopub.execute_input":"2025-03-17T12:33:11.973354Z","iopub.status.idle":"2025-03-17T12:33:11.980952Z","shell.execute_reply.started":"2025-03-17T12:33:11.973311Z","shell.execute_reply":"2025-03-17T12:33:11.979562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"Default\"] = df[\"Default\"].astype(int)  # Ensure it's an integer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:12.733225Z","iopub.execute_input":"2025-03-17T12:33:12.733605Z","iopub.status.idle":"2025-03-17T12:33:12.739671Z","shell.execute_reply.started":"2025-03-17T12:33:12.733575Z","shell.execute_reply":"2025-03-17T12:33:12.738378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X.isnull().sum())  # Check number of missing values per column","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:13.381177Z","iopub.execute_input":"2025-03-17T12:33:13.381565Z","iopub.status.idle":"2025-03-17T12:33:13.399472Z","shell.execute_reply.started":"2025-03-17T12:33:13.381533Z","shell.execute_reply":"2025-03-17T12:33:13.398201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X.columns)  # See all available columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:15.339392Z","iopub.execute_input":"2025-03-17T12:33:15.339778Z","iopub.status.idle":"2025-03-17T12:33:15.345488Z","shell.execute_reply.started":"2025-03-17T12:33:15.339747Z","shell.execute_reply":"2025-03-17T12:33:15.344236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X.dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:16.347283Z","iopub.execute_input":"2025-03-17T12:33:16.347658Z","iopub.status.idle":"2025-03-17T12:33:16.354555Z","shell.execute_reply.started":"2025-03-17T12:33:16.34763Z","shell.execute_reply":"2025-03-17T12:33:16.353394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X[\"Education\"].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:17.153196Z","iopub.execute_input":"2025-03-17T12:33:17.153571Z","iopub.status.idle":"2025-03-17T12:33:17.160758Z","shell.execute_reply.started":"2025-03-17T12:33:17.153544Z","shell.execute_reply":"2025-03-17T12:33:17.159282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df[\"Education\"].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:19.649383Z","iopub.execute_input":"2025-03-17T12:33:19.649724Z","iopub.status.idle":"2025-03-17T12:33:19.657262Z","shell.execute_reply.started":"2025-03-17T12:33:19.649699Z","shell.execute_reply":"2025-03-17T12:33:19.655947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reload the dataset \ndf_original = pd.read_csv(\"/kaggle/input/loan-default/Loan_default.csv\")  \n\n# Check unique values in 'Education'\nprint(df_original[\"Education\"].unique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:21.126476Z","iopub.execute_input":"2025-03-17T12:33:21.126844Z","iopub.status.idle":"2025-03-17T12:33:21.773317Z","shell.execute_reply.started":"2025-03-17T12:33:21.126813Z","shell.execute_reply":"2025-03-17T12:33:21.772142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the correct mapping\neducation_order = {\"High School\": 0, \"Bachelor's\": 1, \"Master's\": 2, \"PhD\": 3}\n\n# Restore the 'Education' column in df\ndf[\"Education\"] = df_original[\"Education\"].map(education_order)\n\n# Fill missing values with the most common category\nmost_frequent_education = df[\"Education\"].mode()[0]\ndf[\"Education\"].fillna(most_frequent_education, inplace=True)\n\n# Convert to integer type\ndf[\"Education\"] = df[\"Education\"].astype(int)\n\n# Assign the fixed column back to X\nX[\"Education\"] = df[\"Education\"]\n\n# Final check\nprint(df[\"Education\"].unique())  # Should be [0, 1, 2, 3]\nprint(df[\"Education\"].isnull().sum())  # Should be 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:22.541274Z","iopub.execute_input":"2025-03-17T12:33:22.541664Z","iopub.status.idle":"2025-03-17T12:33:22.572011Z","shell.execute_reply.started":"2025-03-17T12:33:22.541635Z","shell.execute_reply":"2025-03-17T12:33:22.570859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"education_order = {\"High School\": 0, \"Bachelor's\": 1, \"Master's\": 2, \"PhD\": 3}\n\n# Re-map the original 'Education' values\ndf[\"Education\"] = df_original[\"Education\"].map(education_order)\n\n# Fill NaN values with the most common category\nmost_frequent_education = df[\"Education\"].mode()[0]\ndf[\"Education\"].fillna(most_frequent_education, inplace=True)\n\n# Ensure it's integer type\ndf[\"Education\"] = df[\"Education\"].astype(int)\n\nprint(df[\"Education\"].unique())  # Should return [0, 1, 2, 3]\nprint(df[\"Education\"].isnull().sum())  # Should return 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:27.168273Z","iopub.execute_input":"2025-03-17T12:33:27.168677Z","iopub.status.idle":"2025-03-17T12:33:27.197309Z","shell.execute_reply.started":"2025-03-17T12:33:27.168648Z","shell.execute_reply":"2025-03-17T12:33:27.196171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"most_frequent_education = X[\"Education\"].mode()[0]  # Get most common value\nX[\"Education\"].fillna(most_frequent_education, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:29.675952Z","iopub.execute_input":"2025-03-17T12:33:29.676348Z","iopub.status.idle":"2025-03-17T12:33:29.68325Z","shell.execute_reply.started":"2025-03-17T12:33:29.676301Z","shell.execute_reply":"2025-03-17T12:33:29.682213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Handling Class Imbalance\nfrom imblearn.over_sampling import SMOTE\n\n# Define features and target\nX = df.drop(columns=[\"Default\"])  # Features\ny = df[\"Default\"]  # Target\n\n# Apply SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\nprint(y_resampled.value_counts())  # Check new class distribution\n# Check class distribution after SMOTE in percentage format\nprint(y_resampled.value_counts(normalize=True) * 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:31.949472Z","iopub.execute_input":"2025-03-17T12:33:31.949822Z","iopub.status.idle":"2025-03-17T12:33:34.331067Z","shell.execute_reply.started":"2025-03-17T12:33:31.949794Z","shell.execute_reply":"2025-03-17T12:33:34.329751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot class distribution after SMOTE\nplt.figure(figsize=(6, 4))\nsns.barplot(x=y_resampled.value_counts().index, \n            y=y_resampled.value_counts(normalize=True) * 100, \n            palette=[\"blue\", \"orange\"])\n\nplt.xlabel(\"Default NO = 0 , yes = 1)\")\nplt.ylabel(\"Percentage (%)\")\nplt.title(\"Class Distribution After SMOTE\")\nplt.ylim(0, 100)\n\n# Show values on bars\nfor i, v in enumerate(y_resampled.value_counts(normalize=True) * 100):\n    plt.text(i, v + 1, f\"{v:.2f}%\", ha=\"center\", fontsize=12)\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T12:33:37.736142Z","iopub.execute_input":"2025-03-17T12:33:37.736519Z","iopub.status.idle":"2025-03-17T12:33:37.914997Z","shell.execute_reply.started":"2025-03-17T12:33:37.736488Z","shell.execute_reply":"2025-03-17T12:33:37.913896Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Modeling Step**  \n\nNow that we have preprocessed the data, we will proceed with building and evaluating machine learning models.  \n\n### **Steps in this Phase:**  \n1. **Splitting the Data:**  \n   - Divide the dataset into training and testing sets.  \n\n2. **Training Models:**  \n   - Train multiple models and compare their performance.  \n\n3. **Hyperparameter Tuning:**  \n   - Optimize the best-performing model for better accuracy.  \n\n4. **Evaluation:**  \n   - Assess the model using various performance metrics.  \n\nLet's begin with splitting the data!\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data into training (80%) and testing (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n\n# Print shapes of resulting sets\nprint(f\"Training set: {X_train.shape}, {y_train.shape}\")\nprint(f\"Testing set: {X_test.shape}, {y_test.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:11:49.773138Z","iopub.execute_input":"2025-03-17T13:11:49.773518Z","iopub.status.idle":"2025-03-17T13:11:50.108164Z","shell.execute_reply.started":"2025-03-17T13:11:49.773488Z","shell.execute_reply":"2025-03-17T13:11:50.106946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Initialize models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42)\n}\n\n# Train and evaluate models\nresults = {}\n\nfor name, model in models.items():\n    print(f\"Training {name}...\")\n    \n    start_time = time.time()  # Start timer\n    \n    model.fit(X_train, y_train)  # Train model\n    y_pred = model.predict(X_test)  # Predict on test set\n    acc = accuracy_score(y_test, y_pred)  # Calculate accuracy\n    \n    end_time = time.time()  # End timer\n    elapsed_time = end_time - start_time  # Compute time taken\n    \n    results[name] = {\"Accuracy\": acc, \"Time (seconds)\": elapsed_time}\n    \n    print(f\"{name} Accuracy: {acc:.4f} | Time Taken: {elapsed_time:.2f} sec\")\n\n# Display results in a sorted manner (by accuracy)\nprint(\"\\nFinal Results:\")\nsorted_results = sorted(results.items(), key=lambda x: x[1][\"Accuracy\"], reverse=True)\nfor name, metrics in sorted_results:\n    print(f\"{name}: Accuracy = {metrics['Accuracy']:.4f}, Time = {metrics['Time (seconds)']:.2f} sec\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:13:17.338215Z","iopub.execute_input":"2025-03-17T13:13:17.338596Z","iopub.status.idle":"2025-03-17T13:17:20.936536Z","shell.execute_reply.started":"2025-03-17T13:13:17.338567Z","shell.execute_reply":"2025-03-17T13:17:20.935444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#before optimizing we need to visualize the three models perfomance frist\nfrom sklearn.metrics import confusion_matrix\n\n# Set up subplots\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Generate confusion matrices for each model\nfor i, (name, model) in enumerate(models.items()):\n    y_pred = model.predict(X_test)  # Get predictions\n    cm = confusion_matrix(y_test, y_pred)  # Compute confusion matrix\n    \n    # Plot confusion matrix\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[i])\n    axes[i].set_title(f\"{name} Confusion Matrix\")\n    axes[i].set_xlabel(\"Predicted Label\")\n    axes[i].set_ylabel(\"True Label\")\n\n# Show plots\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:17:55.407182Z","iopub.execute_input":"2025-03-17T13:17:55.407576Z","iopub.status.idle":"2025-03-17T13:18:00.295449Z","shell.execute_reply.started":"2025-03-17T13:17:55.407546Z","shell.execute_reply":"2025-03-17T13:18:00.294214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lets optimize RF model\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport time\nimport numpy as np\n\n# Define improved hyperparameter grid\nparam_grid = {\n    \"n_estimators\": [150, 200, 250, 300],  # More trees for better stability\n    \"max_depth\": [20, 30, None],  # Allow deeper trees\n    \"min_samples_split\": [2, 5],  # Keep splits minimal\n    \"min_samples_leaf\": [1, 2]  # Maintain small leaf sizes\n}\n\n# Initialize Random Forest model\nrf = RandomForestClassifier(random_state=42)\n\n# Use Halving Grid Search for structured tuning\nhalving_search = HalvingGridSearchCV(\n    rf, param_grid=param_grid,  \n    factor=2,  # Reduce candidates gradually\n    min_resources=\"smallest\",  # Automatically choose minimal starting data\n    cv=2,  # Reduce cross-validation folds for speed\n    scoring=\"accuracy\", \n    n_jobs=-1,  # Utilize all CPU cores\n    verbose=1,  # Reduce output verbosity\n    random_state=42\n)\n\n# Start timing\nstart_time = time.time()\nhalving_search.fit(X_train, y_train)\nend_time = time.time()\n\n# Get best model and hyperparameters\nbest_rf = halving_search.best_estimator_\nbest_params = halving_search.best_params_\n\n# Evaluate model on test set\ny_pred = best_rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\n# Print results\nprint(\"\\nBest Hyperparameters:\", best_params)\nprint(f\"Optimized Random Forest Accuracy: {accuracy:.4f} | Time Taken: {end_time - start_time:.2f} sec\")\n","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:19:13.794694Z","iopub.execute_input":"2025-03-17T13:19:13.795084Z","iopub.status.idle":"2025-03-17T13:24:31.846264Z","shell.execute_reply.started":"2025-03-17T13:19:13.795053Z","shell.execute_reply":"2025-03-17T13:24:31.844957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lets train xgboost as  it is better and faster than gradient boosting \nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Define XGBoost Classifier\nxgb_clf = xgb.XGBClassifier(\n    objective='binary:logistic',\n    eval_metric='logloss',\n    use_label_encoder=False\n)\n\n# Define hyperparameter grid\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 6, 10],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0],\n}\n\n# Use HalvingGridSearchCV for faster hyperparameter tuning\ngrid_search = HalvingGridSearchCV(\n    xgb_clf, param_grid, factor=2, cv=3, scoring='accuracy', n_jobs=-1, verbose=2\n)\n\nstart_time = time.time()\ngrid_search.fit(X_train, y_train)\nend_time = time.time()\n\n# Get best parameters\nbest_params = grid_search.best_params_\nprint(\"Best Parameters for XGBoost:\", best_params)\n\n# Train best model without early stopping\nbest_xgb = xgb.XGBClassifier(**best_params, eval_metric='logloss', use_label_encoder=False)\nbest_xgb.fit(X_train, y_train)\n\n# Predict\ny_pred = best_xgb.predict(X_test)\n\n# Accuracy\nxgb_accuracy = accuracy_score(y_test, y_pred)\nelapsed_time = end_time - start_time\n\nprint(f\"Optimized XGBoost Accuracy: {xgb_accuracy:.4f} | Time Taken: {elapsed_time:.2f} sec\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:24:50.111556Z","iopub.execute_input":"2025-03-17T13:24:50.111926Z","iopub.status.idle":"2025-03-17T13:27:05.830423Z","shell.execute_reply.started":"2025-03-17T13:24:50.111894Z","shell.execute_reply":"2025-03-17T13:27:05.829469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lets visualize after optimizing the two models\n\n# Replace these with actual values from your optimized models\nrf_accuracy = 0.9084  # Optimized Random Forest accuracy\nxgb_accuracy = 0.9146  # Optimized XGBoost accuracy\n\n# Model names and their accuracies\nmodels = [\"Random Forest\", \"XGBoost\"]\naccuracies = [rf_accuracy, xgb_accuracy]\n\n# Create the plot\nplt.figure(figsize=(6, 4))\nsns.barplot(x=models, y=accuracies, palette=\"viridis\")\n\n# Add labels and title\nplt.ylim(0.7, 1)  # Set y-axis range\nplt.ylabel(\"Accuracy\")\nplt.title(\"Comparison of Optimized Models\")\n\n# Show values on bars\nfor i, acc in enumerate(accuracies):\n    plt.text(i, acc + 0.005, f\"{acc:.4f}\", ha='center', fontsize=12)\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:27:31.410028Z","iopub.execute_input":"2025-03-17T13:27:31.410409Z","iopub.status.idle":"2025-03-17T13:27:31.591292Z","shell.execute_reply.started":"2025-03-17T13:27:31.410378Z","shell.execute_reply":"2025-03-17T13:27:31.590204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lets visalize with confusion matrix \nfrom sklearn.metrics import confusion_matrix\n\n# Get predictions for both models\nrf_y_pred = best_rf.predict(X_test)\nxgb_y_pred = best_xgb.predict(X_test)\n\n# Compute confusion matrices\nrf_cm = confusion_matrix(y_test, rf_y_pred)\nxgb_cm = confusion_matrix(y_test, xgb_y_pred)\n\n# Plot confusion matrices\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nsns.heatmap(rf_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Default\", \"Default\"], yticklabels=[\"No Default\", \"Default\"], ax=axes[0])\naxes[0].set_title(\"Random Forest Confusion Matrix\")\naxes[0].set_xlabel(\"Predicted\")\naxes[0].set_ylabel(\"Actual\")\n\nsns.heatmap(xgb_cm, annot=True, fmt=\"d\", cmap=\"Oranges\", xticklabels=[\"No Default\", \"Default\"], yticklabels=[\"No Default\", \"Default\"], ax=axes[1])\naxes[1].set_title(\"XGBoost Confusion Matrix\")\naxes[1].set_xlabel(\"Predicted\")\naxes[1].set_ylabel(\"Actual\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:27:40.689564Z","iopub.execute_input":"2025-03-17T13:27:40.689908Z","iopub.status.idle":"2025-03-17T13:27:50.218391Z","shell.execute_reply.started":"2025-03-17T13:27:40.689882Z","shell.execute_reply":"2025-03-17T13:27:50.217257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\n\n# Plot feature importance\nxgb.plot_importance(best_xgb)\nplt.title(\"Feature Importance - XGBoost\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:33:05.106562Z","iopub.execute_input":"2025-03-17T13:33:05.106947Z","iopub.status.idle":"2025-03-17T13:33:05.522941Z","shell.execute_reply.started":"2025-03-17T13:33:05.106919Z","shell.execute_reply":"2025-03-17T13:33:05.52155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, precision_recall_curve\n\n# ROC Curve\ny_prob = best_xgb.predict_proba(X_test)[:, 1]\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6,5))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve - XGBoost\")\nplt.legend()\nplt.show()\n\n# Precision-Recall Curve\nprecision, recall, _ = precision_recall_curve(y_test, y_prob)\nplt.figure(figsize=(6,5))\nplt.plot(recall, precision, label=\"Precision-Recall Curve\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve - XGBoost\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:33:10.232721Z","iopub.execute_input":"2025-03-17T13:33:10.233059Z","iopub.status.idle":"2025-03-17T13:33:11.044608Z","shell.execute_reply.started":"2025-03-17T13:33:10.23303Z","shell.execute_reply":"2025-03-17T13:33:11.043543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Get predictions\ny_pred = best_xgb.predict(X_test)\n\n# Compute metrics\nmetrics = {\n    \"Accuracy\": accuracy_score(y_test, y_pred),\n    \"Precision\": precision_score(y_test, y_pred),\n    \"Recall\": recall_score(y_test, y_pred),\n    \"F1-score\": f1_score(y_test, y_pred)\n}\n\n# Plot metrics\nplt.figure(figsize=(8, 6))\nplt.bar(metrics.keys(), metrics.values(), color=['blue', 'green', 'orange', 'red'])\n\n# Add value labels\nfor i, v in enumerate(metrics.values()):\n    plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontsize=12)\n\n# Labels and title\nplt.ylabel(\"Score\")\nplt.ylim(0, 1)  # Scores are between 0 and 1\nplt.title(\"Model Performance Metrics (XGBoost)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:33:22.703232Z","iopub.execute_input":"2025-03-17T13:33:22.703625Z","iopub.status.idle":"2025-03-17T13:33:23.230441Z","shell.execute_reply.started":"2025-03-17T13:33:22.703594Z","shell.execute_reply":"2025-03-17T13:33:23.229236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Print the features in X before saving\nprint(f\" Number of features in X: {X.shape[1]}\")\nprint(f\" Feature names in X: {list(X.columns)}\")\n\n# Check if there were any issues with one-hot encoding\nprint(f\" Unique values in y: {y.unique()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:33:49.45288Z","iopub.execute_input":"2025-03-17T13:33:49.453247Z","iopub.status.idle":"2025-03-17T13:33:49.461575Z","shell.execute_reply.started":"2025-03-17T13:33:49.453217Z","shell.execute_reply":"2025-03-17T13:33:49.460359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Save the correct feature names\ncolumns_used = X.columns\njoblib.dump(columns_used, \"columns_used.pkl\")\nprint(\" Correct feature names saved in 'columns_used.pkl'!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:33:51.947624Z","iopub.execute_input":"2025-03-17T13:33:51.948042Z","iopub.status.idle":"2025-03-17T13:33:51.955154Z","shell.execute_reply.started":"2025-03-17T13:33:51.948008Z","shell.execute_reply":"2025-03-17T13:33:51.953802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the features in X before saving the scaler\nprint(f\"Number of features in X (before saving scaler): {X.shape[1]}\")\nprint(f\"Feature names in X: {list(X.columns)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:33:53.946065Z","iopub.execute_input":"2025-03-17T13:33:53.94648Z","iopub.status.idle":"2025-03-17T13:33:53.951275Z","shell.execute_reply.started":"2025-03-17T13:33:53.946449Z","shell.execute_reply":"2025-03-17T13:33:53.950149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport joblib\n\n# Train a new StandardScaler with the correct features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)  # Use the correct X\n\n# Save the new scaler\njoblib.dump(scaler, \"scaler.pkl\")\nprint(\" Scaler has been correctly trained and saved as 'scaler.pkl'!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:33:55.736614Z","iopub.execute_input":"2025-03-17T13:33:55.737014Z","iopub.status.idle":"2025-03-17T13:33:56.057257Z","shell.execute_reply.started":"2025-03-17T13:33:55.736972Z","shell.execute_reply":"2025-03-17T13:33:56.055769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\njoblib.dump(model, 'xgboost_model.pkl')\nprint ('xgboost_model.pkl sved successfully')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T13:34:00.384941Z","iopub.execute_input":"2025-03-17T13:34:00.385315Z","iopub.status.idle":"2025-03-17T13:34:00.401491Z","shell.execute_reply.started":"2025-03-17T13:34:00.385283Z","shell.execute_reply":"2025-03-17T13:34:00.400383Z"}},"outputs":[],"execution_count":null}]}